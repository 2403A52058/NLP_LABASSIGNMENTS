{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPanbYes0fiQc+vt/EcsFsZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2403A52058/NLP_LABASSIGNMENTS/blob/main/NLP_LAB(08)_2403A52058.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Required Libraries"
      ],
      "metadata": {
        "id": "xRCiQE9F73hK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3hQ7IJ26fsE",
        "outputId": "3bc40048-471f-4e99-fd28-01cb49e646fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Import nltk for NLP utilities like tokenization\n",
        "import nltk\n",
        "\n",
        "# Import re for text cleaning using regular expressions\n",
        "import re\n",
        "\n",
        "# Import Counter for counting words and n-grams\n",
        "from collections import Counter\n",
        "\n",
        "# Import numpy for mathematical operations\n",
        "import numpy as np\n",
        "\n",
        "# Download tokenizer resources (run once)\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "lJAqF0bp7_rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the dataset text file in read mode\n",
        "with open(\"/content/corpus.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "\n",
        "    # Read entire content of file\n",
        "    text = file.read()\n",
        "\n",
        "# Print first 500 characters to verify data\n",
        "print(text[:500])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt4FGSvA7_VB",
        "outputId": "d813e025-b0bf-4030-9e10-0dc25b6449e3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural language processing is a field of artificial intelligence that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate text in a meaningful way. Language models play a vital role in natural language processing by predicting the likelihood of a sequence of words. These models are widely used in applications such as speech recognition, machine translation, and text generation.\n",
            "\n",
            "Language modeling is based on the idea that w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing"
      ],
      "metadata": {
        "id": "NynbIkmz_fq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all text to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Remove punctuation and numbers\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "# Tokenize text into words\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Print first 20 tokens to verify\n",
        "print(tokens[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm0XC9Th_kB-",
        "outputId": "0acdf21d-25f1-4f0c-ac92-63d0f29b1043"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'language', 'it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-Test Split"
      ],
      "metadata": {
        "id": "EGkuOFbr_noN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate split index for 80% training\n",
        "split_index = int(0.8 * len(tokens))\n",
        "\n",
        "# Split tokens into training data\n",
        "train_tokens = tokens[:split_index]\n",
        "\n",
        "# Split tokens into testing data\n",
        "test_tokens = tokens[split_index:]\n"
      ],
      "metadata": {
        "id": "p-hNgqs8_rMN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Unigram Model"
      ],
      "metadata": {
        "id": "rkMLNuhs_yLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count frequency of each word in training data\n",
        "unigram_counts = Counter(train_tokens)\n",
        "\n",
        "# Calculate total number of words\n",
        "total_unigrams = sum(unigram_counts.values())\n",
        "\n",
        "# Create unigram probability dictionary\n",
        "unigram_probs = {}\n",
        "\n",
        "# Loop through each word and its count\n",
        "for word, count in unigram_counts.items():\n",
        "\n",
        "    # Calculate probability of each word\n",
        "    unigram_probs[word] = count / total_unigrams\n"
      ],
      "metadata": {
        "id": "IIiw8VZf_1Yt"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Bigram Model"
      ],
      "metadata": {
        "id": "E3sGPBCl_6Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create bigrams from training tokens\n",
        "bigrams = list(zip(train_tokens[:-1], train_tokens[1:]))\n",
        "\n",
        "# Count bigram frequencies\n",
        "bigram_counts = Counter(bigrams)\n",
        "\n",
        "# Create bigram probability dictionary\n",
        "bigram_probs = {}\n",
        "\n",
        "# Loop through each bigram\n",
        "for (w1, w2), count in bigram_counts.items():\n",
        "\n",
        "    # Divide bigram count by unigram count\n",
        "    bigram_probs[(w1, w2)] = count / unigram_counts[w1]\n"
      ],
      "metadata": {
        "id": "XdgrP-Fy_6g9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Trigram Model"
      ],
      "metadata": {
        "id": "rY3WnirNAG4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create trigrams from training tokens\n",
        "trigrams = list(zip(train_tokens[:-2], train_tokens[1:-1], train_tokens[2:]))\n",
        "\n",
        "# Count trigram frequencies\n",
        "trigram_counts = Counter(trigrams)\n",
        "\n",
        "# Create trigram probability dictionary\n",
        "trigram_probs = {}\n",
        "\n",
        "# Loop through each trigram\n",
        "for (w1, w2, w3), count in trigram_counts.items():\n",
        "\n",
        "    # Divide trigram count by bigram count\n",
        "    trigram_probs[(w1, w2, w3)] = count / bigram_counts[(w1, w2)]\n"
      ],
      "metadata": {
        "id": "pkjBa3AoAHcN"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add-One (Laplace) Smoothing"
      ],
      "metadata": {
        "id": "afgjIyAMAOt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary size\n",
        "V = len(unigram_counts)\n",
        "\n",
        "# Define smoothed bigram probability function\n",
        "def smoothed_bigram_prob(w1, w2):\n",
        "\n",
        "    # Get bigram count with default 0\n",
        "    bigram_count = bigram_counts.get((w1, w2), 0)\n",
        "\n",
        "    # Apply Laplace smoothing formula\n",
        "    return (bigram_count + 1) / (unigram_counts[w1] + V)\n"
      ],
      "metadata": {
        "id": "qH08aaRXAPJM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Probability (Unigram)"
      ],
      "metadata": {
        "id": "Ypx0MHjWAYB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute unigram sentence probability\n",
        "def unigram_sentence_prob(sentence):\n",
        "\n",
        "    # Tokenize sentence\n",
        "    words = nltk.word_tokenize(sentence.lower())\n",
        "\n",
        "    # Initialize probability\n",
        "    prob = 1\n",
        "\n",
        "    # Loop through each word\n",
        "    for word in words:\n",
        "\n",
        "        # Multiply word probability (use small value if unseen)\n",
        "        prob *= unigram_probs.get(word, 1e-6)\n",
        "\n",
        "    return prob\n"
      ],
      "metadata": {
        "id": "FUVoBQfZAb78"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Probability (Bigram)"
      ],
      "metadata": {
        "id": "hkNgAlPVAfXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute bigram sentence probability\n",
        "def bigram_sentence_prob(sentence):\n",
        "\n",
        "    # Tokenize sentence\n",
        "    words = nltk.word_tokenize(sentence.lower())\n",
        "\n",
        "    # Initialize probability\n",
        "    prob = 1\n",
        "\n",
        "    # Loop through bigrams\n",
        "    for i in range(len(words) - 1):\n",
        "\n",
        "        # Multiply smoothed bigram probability\n",
        "        prob *= smoothed_bigram_prob(words[i], words[i+1])\n",
        "\n",
        "    return prob\n"
      ],
      "metadata": {
        "id": "dExkmfN5AgHc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perplexity Calculation"
      ],
      "metadata": {
        "id": "kQhe3aqJAobU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate perplexity\n",
        "def perplexity(sentence, model_func):\n",
        "\n",
        "    # Tokenize sentence\n",
        "    words = nltk.word_tokenize(sentence.lower())\n",
        "\n",
        "    # Calculate sentence probability\n",
        "    prob = model_func(sentence)\n",
        "\n",
        "    # Compute perplexity formula\n",
        "    return pow(1 / prob, 1 / len(words))\n"
      ],
      "metadata": {
        "id": "u53xTGQnAuKM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Sentences"
      ],
      "metadata": {
        "id": "6IMfN2AEA0Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of test sentences\n",
        "sentences = [\n",
        "    \"language models are useful\",\n",
        "    \"this is a simple example\",\n",
        "    \"n gram models predict words\",\n",
        "    \"machine learning is powerful\",\n",
        "    \"unseen words cause problems\"\n",
        "]\n",
        "\n",
        "# Loop through sentences and print perplexity\n",
        "for s in sentences:\n",
        "\n",
        "    print(\"Sentence:\", s)\n",
        "\n",
        "    print(\"Unigram Perplexity:\", perplexity(s, unigram_sentence_prob))\n",
        "\n",
        "    print(\"Bigram Perplexity:\", perplexity(s, bigram_sentence_prob))\n",
        "\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM_JXKvHA8lM",
        "outputId": "a8e01a54-c447-4d0f-9578-12129a4c74ee"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: language models are useful\n",
            "Unigram Perplexity: 88.54595240526768\n",
            "Bigram Perplexity: 34.31120589604889\n",
            "--------------------------------------------------\n",
            "Sentence: this is a simple example\n",
            "Unigram Perplexity: 519.831225360002\n",
            "Bigram Perplexity: 63.07148173600104\n",
            "--------------------------------------------------\n",
            "Sentence: n gram models predict words\n",
            "Unigram Perplexity: 765.4594600044012\n",
            "Bigram Perplexity: 60.658218456421935\n",
            "--------------------------------------------------\n",
            "Sentence: machine learning is powerful\n",
            "Unigram Perplexity: 1749.3426073136952\n",
            "Bigram Perplexity: 59.697599999987254\n",
            "--------------------------------------------------\n",
            "Sentence: unseen words cause problems\n",
            "Unigram Perplexity: 13866.018957806076\n",
            "Bigram Perplexity: 59.49551913027847\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}